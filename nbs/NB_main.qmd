---
title: "Lawmaking Coordination? Explaining Referral Failure in Federated Judicial Systems"
author: "Mauricio M. M."
date: 2024.07.17
description: This document was created for documentation puproses so that the user has an intuitive understanding of how do the principles of coding in R relate to those in Python.
format: 
  html:
    theme: journal
    code-copy: true
    toc: true
    toc-location: right 
execute: 
    echo: true
    eval: true
html-math-method: mathjax
---

# NB1: Notebook created for scraping all the applications text from the ECJ

**Output:** Clean dataset with the following variables. 

- [ ] `decision_date`: application date. 
- [ ] `referral_date `: date when the reference for preliminary referenc was sent. 
- [ ] `referring_court_name`: ???
- [ ] `ms_origin`: ???
- [ ] `num_applications`: def() construct from using `iuropa_all_cases_id` same for all apps.
- [ ] `full_text`: Full applications text that can be paresed into paragraphs and citations.
- [ ] `par_location`:
- [ ] `par_location_text`: 
- [ ] `cited_celex`:
- [ ] `cited_locations`:
- [ ] `cited_iuropa_id`: Code created using the constructor `A000P000L000`
- [ ] `num_questions`: def(max(par_locations)) unique for each app.
- [ ] `firts_referral`: def (sort_by(decision_date)) create bool flag.

**Output**: Create script that scrapes updates

## Environment and Settings Python

```{python}
# Required Libraries 
import os 
import sys
from bs4 import BeautifulSoup
# Add project paths for for easier access across whole project
#paths_script_path = "C:\\Users\\mauricmm\\iCloudDrive\\cloudgit\\uio24emc\\"
parent_dir = os.path.dirname(os.getcwd(),) +"\\"
sys.path.append(parent_dir)
import paths
# Access paths globally
paths.figures_dir
```

```{python}


# def cool_function(x):

```

```{python}
# # Specify the file path
# file_path = "municipal_elections/oslo_fp.html"
# # Read HTML content from the file
# with open(file_path, 'r', encoding='UTF-8') as file:
#     html_content = file.read()

# # Parse the HTML content with BeautifulSoup
# soup = BeautifulSoup(html_content, 'html.parser')

# # Extract links using BeautifulSoup
# links = soup.select("table tr th a[href]")
# link_urls = [link['href'] for link in links]


```

## Environment and Settings R

```{r}

```

### DAG Diagram

```{r}

```